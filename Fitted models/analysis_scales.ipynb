{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"13sspqiEZwso4NYTbsflpPyNFaVAAxUgr","timestamp":1715367937314},{"file_id":"1RU2D0OfUp9vkpPnZGN1pSUrVtf_HcvPz","timestamp":1672185517864},{"file_id":"1TAAi_szMfWqRfHVfjGSqnGVLr_ztzUM9","timestamp":1670007363628},{"file_id":"1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0","timestamp":1623440015685},{"file_id":"1XTKHiIcvyL5nuldx0HSL_dUa8yopzy_Y","timestamp":1568928635382},{"file_id":"1gUnPS2zuUOUe4YG-2iDm_Y2X5RTkgsGh","timestamp":1556293046020}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"scfLT2i0MLyD"},"source":["# Environment Sanity Check #\n","\n","Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n","\n","You can check the output of `!nvidia-smi` to check which GPU you have.  Please uncomment the cell below if you'd like to do that.  Currently, RAPIDS runs on all available Colab GPU instances."]},{"cell_type":"code","metadata":{"id":"67T0090Jk2KL","executionInfo":{"status":"ok","timestamp":1715415305948,"user_tz":-120,"elapsed":26,"user":{"displayName":"Harm Tanis","userId":"15239638384700492892"}},"outputId":"4094efc5-5cce-4ce0-a2da-8a02e3d9bfbb","colab":{"base_uri":"https://localhost:8080/"}},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}]},{"cell_type":"markdown","metadata":{"id":"U_v33LnDVNo3"},"source":["#Setup:\n","This set up script:\n","\n","1. Checks to make sure that the GPU is RAPIDS compatible\n","1. Installs the **current stable version** of RAPIDSAI's core libraries using pip, which are:\n","  1. cuDF\n","  1. cuML\n","  1. cuGraph\n","  1. cuSpatial\n","  1. cuxFilter\n","  1. cuCIM\n","  1. xgboost\n","\n","**This will complete in about 5-6 minutes**\n","\n","If you require installing the **nightly** releases of RAPIDSAI, please use the [RAPIDS Conda Colab Template notebook](https://colab.research.google.com/drive/1TAAi_szMfWqRfHVfjGSqnGVLr_ztzUM9) and use the nightly parameter option when running the RAPIDS installation cell.\n"]},{"cell_type":"code","metadata":{"id":"B0C8IV5TQnjN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715415329153,"user_tz":-120,"elapsed":15501,"user":{"displayName":"Harm Tanis","userId":"15239638384700492892"}},"outputId":"1fb8d5e8-aef4-4dc5-a3e0-523cf9c61aaa"},"source":["# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n","# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n","!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n","!python rapidsai-csp-utils/colab/pip-install.py\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'rapidsai-csp-utils'...\n","remote: Enumerating objects: 476, done.\u001b[K\n","remote: Counting objects: 100% (207/207), done.\u001b[K\n","remote: Compressing objects: 100% (116/116), done.\u001b[K\n","remote: Total 476 (delta 141), reused 124 (delta 91), pack-reused 269\u001b[K\n","Receiving objects: 100% (476/476), 131.59 KiB | 1.34 MiB/s, done.\n","Resolving deltas: 100% (243/243), done.\n","Collecting pynvml\n","  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 820.9 kB/s eta 0:00:00\n","Installing collected packages: pynvml\n","Successfully installed pynvml-11.5.0\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 1798, in _LoadNvmlLibrary\n","    nvmlLib = CDLL(\"libnvidia-ml.so.1\")\n","  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n","    self._handle = _dlopen(self._name, mode)\n","OSError: libnvidia-ml.so.1: cannot open shared object file: No such file or directory\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/rapidsai-csp-utils/colab/pip-install.py\", line 18, in <module>\n","    pynvml.nvmlInit()\n","  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 1770, in nvmlInit\n","    nvmlInitWithFlags(0)\n","  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 1753, in nvmlInitWithFlags\n","    _LoadNvmlLibrary()\n","  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 1800, in _LoadNvmlLibrary\n","    _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)\n","  File \"/usr/local/lib/python3.10/dist-packages/pynvml/nvml.py\", line 833, in _nvmlCheckReturn\n","    raise NVMLError(ret)\n","pynvml.nvml.NVMLError_LibraryNotFound: NVML Shared Library Not Found\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/rapidsai-csp-utils/colab/pip-install.py\", line 20, in <module>\n","    raise Exception(\"\"\"\n","Exception: \n","                  Unfortunately you're in a Colab instance that doesn't have a GPU.\n","\n","                  Please make sure you've configured Colab to request a GPU Instance Type.\n","               \n","                  Go to 'Runtime -> Change Runtime Type --> under the Hardware Accelerator, select GPU', then try again.\n"]}]},{"cell_type":"markdown","metadata":{"id":"pZJMJ6BulmMn"},"source":["# RAPIDS is now installed on Colab.  \n","You can copy your code into the cells below or use the below to validate your RAPIDS installation and version.  \n","# Enjoy!"]},{"cell_type":"code","metadata":{"id":"4nLrk46BllED","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1715366453058,"user_tz":-120,"elapsed":5098,"user":{"displayName":"Harm Tanis","userId":"17777912620782591797"}},"outputId":"b979d01e-ba65-447b-dc20-d5cd51e7fa7f"},"source":["import cudf\n","cudf.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'24.04.01'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import cuml\n","cuml.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"xgAFgI15ddf6","executionInfo":{"status":"ok","timestamp":1715366458678,"user_tz":-120,"elapsed":3544,"user":{"displayName":"Harm Tanis","userId":"17777912620782591797"}},"outputId":"5ab83280-9db8-4e9c-d472-9b941c59aa9b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'24.04.00'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import cugraph\n","cugraph.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"JOCMWaUal1fI","executionInfo":{"status":"ok","timestamp":1701366660269,"user_tz":480,"elapsed":12542,"user":{"displayName":"Taurean Dyer US","userId":"13839179491004401894"}},"outputId":"e67a6b6d-b529-486c-cb86-2bb5f8229503"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'23.10.00'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import cuspatial\n","cuspatial.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"AnmtYjzvVTtv","executionInfo":{"status":"ok","timestamp":1701366660641,"user_tz":480,"elapsed":374,"user":{"displayName":"Taurean Dyer US","userId":"13839179491004401894"}},"outputId":"f82dc8cd-8622-46ea-b337-c647fdb93a2d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'23.10.00'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import cuxfilter\n","cuxfilter.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"CYjcARDFVWWD","executionInfo":{"status":"ok","timestamp":1701366663223,"user_tz":480,"elapsed":2585,"user":{"displayName":"Taurean Dyer US","userId":"13839179491004401894"}},"outputId":"e323a8f1-9582-4ff0-cd43-2a3a23eb3b3e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'23.10.00'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Dlsyk9m9NN2K"},"source":["# Next Steps #\n","\n","For an overview of how you can access and work with your own datasets in Colab, check out [this guide](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92).\n","\n","For more RAPIDS examples, check out our RAPIDS notebooks repos:\n","1. https://github.com/rapidsai/notebooks\n","2. https://github.com/rapidsai/notebooks-contrib"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","data_source = \"drive/MyDrive/Thesis Nina\"\n","import os\n","print(os.listdir(data_source))"],"metadata":{"id":"fjXVuk-Xz1uV","executionInfo":{"status":"ok","timestamp":1715371634703,"user_tz":-120,"elapsed":1987,"user":{"displayName":"Harm Tanis","userId":"15239638384700492892"}},"outputId":"c7397bc8-500c-4163-ec2d-d1b78c22f51c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","['scales']\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import os\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n","from sklearn.ensemble import RandomForestRegressor\n","from cuml.ensemble import RandomForestRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.exceptions import ConvergenceWarning\n","\n","from sklearn.feature_selection import SelectFromModel\n","from sklearn.svm import SVR\n","from sklearn.linear_model import Ridge, Lasso, ElasticNet\n","from sklearn.pipeline import make_pipeline\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","import time\n","\n","import warnings\n","\n","X_train = pd.read_pickle(f'{data_source}/scale_data/X_train.pkl').astype('float32')\n","X_val = pd.read_pickle(f'{data_source}/scale_data/X_val.pkl').astype('float32')\n","X_test = pd.read_pickle(f'{data_source}/scale_data/X_test.pkl').astype('float32')\n","y_train = pd.read_pickle(f'{data_source}/scale_data/y_train.pkl').astype('float32')\n","y_val = pd.read_pickle(f'{data_source}/scale_data/y_val.pkl').astype('float32')\n","y_test = pd.read_pickle(f'{data_source}/scale_data/y_test.pkl').astype('float32')\n","\n","models = {\n","\t'Random forest regressor': make_pipeline(\n","\t\tRandomForestRegressor(split_criterion=1000, max_features='sqrt', random_state=42, n_streams=1)\n","\t),\n","\t# 'Random forest regressor fitted': make_pipeline(\n","\t# \tRandomForestRegressor(1000, max_features='sqrt', random_state=42, max_depth=12)\n","\t# ),\n","\t'Decision tree regressor': make_pipeline(\n","\t\tDecisionTreeRegressor()\n","\t),\n","\t'Ridge': make_pipeline(\n","\t\tRidge()\n","\t),\n","\t'Lasso': make_pipeline(\n","\t\tLasso()\n","\t),\n","\t'ElasticNet': make_pipeline(\n","\t\tElasticNet(max_iter=10000)\n","\t),\n","\t'Support vector machine': make_pipeline(\n","\t\tSVR()\n","\t),\n","\t'PCA + Ridge': make_pipeline(\n","\t\tPCA(),\n","\t\tRidge(),\n","\t),\n","\t\"PCA + random forest regressor\": make_pipeline(\n","\t\tPCA(),\n","\t\tRandomForestRegressor(1000, max_depth=12, max_features='sqrt', random_state=42, n_streams=1)\n","\t),\n","\t# \"SelectFromModel + random forest regressor\": make_pipeline(\n","\t# \tSelectFromModel(RandomForestRegressor(n_estimators=100, random_state=42, n_streams=1)),\n","\t# \tRandomForestRegressor(1000, max_depth=12, max_features='sqrt', random_state=42, n_streams=1)\n","\t# ),\n","\t# \"SelectFromModel + random forest regressor fitted\": make_pipeline(\n","\t# \tSelectFromModel(RandomForestRegressor(n_estimators=100, random_state=42)),\n","\t# \tRandomForestRegressor(1000, max_depth=12, max_features='sqrt', random_state=42)\n","\t# )\n","}\n","\n","param_grids = {\n","\t'Random forest regressor': {\n","\t\t'randomforestregressor__max_depth': [*np.linspace(1, 21, 21).astype(int)]\n","\t},\n","\t'Decision tree regressor': {\n","\t\t'decisiontreeregressor__max_depth': [*np.linspace(1, 21, 21).astype(int)]\n","\t},\n","\t'Ridge': {\n","\t\t'ridge__alpha': np.logspace(-3, 3, 7)\n","\t},\n","\t'Lasso': {\n","\t\t'lasso__alpha': np.logspace(-3, 3, 7)\n","\t},\n","\t'ElasticNet': {\n","\t\t\"elasticnet__alpha\": np.logspace(-3, 3, 7),\n","\t\t\"elasticnet__l1_ratio\": np.linspace(0, 1, 11),\n","\t},\n","\t'Support vector machine': {\n","\t\t\"svr__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n","\t},\n","\t'PCA + Ridge': {\n","\t\t\"pca__n_components\": np.linspace(1, X_train.shape[1], X_train.shape[1]).astype(int),\n","\t\t'ridge__alpha': np.logspace(-3, 3, 7)\n","\t},\n","\t'PCA + random forest regressor': {\n","\t\t\"pca__n_components\": np.linspace(1, X_train.shape[1], X_train.shape[1]).astype(int),\n","\t\t'randomforestregressor__max_depth': [*np.linspace(1, 21, 21).astype(int)]\n","\t},\n","\t\"SelectFromModel + random forest regressor\": {\n","\t\t\"selectfrommodel__max_features\": np.linspace(1, X_train.shape[1], X_train.shape[1]).astype(int),\n","\t\t'randomforestregressor__max_depth': [*np.linspace(1, 21, 21).astype(int)]\n","\t}\n","\n","}\n","\n","\n","data = {\n","    \"Model\": [\"Random forest regressor\", \"Decision tree regressor\", \"PCA + random forest regressor\"],\n","    \"DASS_anxiety Optimal parameters\": [\"max. depth=12\", \"\", \"n_components\"],\n","    \"DASS_anxiety Training score MSE\": [\"MSE_value\", \"MSE_value\", \"MSE_value\"],\n","    \"DASS_anxiety Training score R2\": [\"R2_value\", \"R2_value\", \"R2_value\"],\n","    \"DASS_anxiety Validation score MSE\": [\"MSE_value\", \"MSE_value\", \"MSE_value\"],\n","    \"DASS_anxiety Validation score R2\": [\"R2_value\", \"R2_value\", \"R2_value\"],\n","    \"DASS_stress Optimal parameters\": [\"max. depth=12\", \"\", \"n_components\"],\n","    \"DASS_stress Training score MSE\": [\"MSE_value\", \"MSE_value\", \"MSE_value\"],\n","    \"DASS_stress Training score R2\": [\"R2_value\", \"R2_value\", \"R2_value\"],\n","    \"DASS_stress Validation score MSE\": [\"MSE_value\", \"MSE_value\", \"MSE_value\"],\n","    \"DASS_stress Validation score R2\": [\"R2_value\", \"R2_value\", \"R2_value\"],\n","    \"DASS_depression Optimal parameters\":  [\"max. depth=12\", \"\", \"n_components\"],\n","    \"DASS_depression Training score MSE\": [\"MSE_value\", \"MSE_value\", \"MSE_value\"],\n","    \"DASS_depression Training score R2\": [\"R2_value\", \"R2_value\", \"R2_value\"],\n","    \"DASS_depression Validation score MSE\": [\"MSE_value\", \"MSE_value\", \"MSE_value\"],\n","    \"DASS_depression Validation score R2\": [\"R2_value\", \"R2_value\", \"R2_value\"],\n","}\n","\n","for key in data:\n","\tdata[key] = []\n","\n","warnings.filterwarnings('ignore', category=ConvergenceWarning)\n","\n","cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","# X_train_np = X_train.to_numpy()\n","# y_train_np = y_train.to_numpy()\n","# X_val_np = X_val.to_numpy()\n","# y_val_np = y_val.to_numpy()\n","# print(models.keys()[])\n","# for model_name in list(models.keys())[2:4]:\n","\n","def write_file(data):\n","\tdf = pd.DataFrame(data)\n","\n","\tdf.columns = pd.MultiIndex.from_tuples([\n","\t\t\t('Model',\"\",\"\"),\n","\t\t\t('DASS_anxiety', 'Optimal parameters', \"\"),\n","\t\t\t('DASS_anxiety', 'Training scores', 'MSE'), ('DASS_anxiety', 'Training scores', 'R2'),\n","\t\t\t('DASS_anxiety', 'Validation scores', 'MSE'), ('DASS_anxiety', 'Validation scores', 'R2'),\n","\t\t\t('DASS_stress', 'Optimal parameters', \"\"),\n","\t\t\t('DASS_stress', 'Training scores', 'MSE'), ('DASS_stress', 'Training scores', 'R2'),\n","\t\t\t('DASS_stress', 'Validation scores', 'MSE'), ('DASS_stress', 'Validation scores', 'R2'),\n","\t\t\t('DASS_depression', 'Optimal parameters', \"\"),\n","\t\t\t('DASS_depression', 'Training scores', 'MSE'), ('DASS_depression', 'Training scores', 'R2'),\n","\t\t\t('DASS_depression', 'Validation scores', 'MSE'), ('DASS_depression', 'Validation scores', 'R2'),\n","\t])\n","\n","\n","\t# Write DataFrame to Excel file\n","\tdf.to_excel(f'{data_source}/model_comparison.xlsx', engine='openpyxl', index=True)\n","\n","for model_name in models:\n","\tmodel = models[model_name]\n","\tprint(model_name)\n","\tdata[\"Model\"].append(model_name)\n","\n","\tfor column in y_train:\n","\t\tt = time.time()\n","\t\tprint(column)\n","\t\ty_train_column, y_val_column = y_train[column], y_val[column]\n","\t\tgrid_search = GridSearchCV(model, param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', verbose=2)\n","\t\tgrid_search.fit(X_train, y_train_column)\n","\n","\t\tbest_params = grid_search.best_params_\n","\t\tdata[f\"{column} Optimal parameters\"].append(\", \".join([f\"{parameter} = {best_params[parameter]}\" for parameter in best_params]))\n","\t\t# print(\"Best cross-validation score (MSE):\", -grid_search.best_score_)\n","\n","\t\t# Predictions on the training and validation sets\n","\t\ty_train_pred = grid_search.predict(X_train)\n","\t\ty_val_pred = grid_search.predict(X_val)\n","\n","\t\t# Calculate MSE for the validation set\n","\t\tmse_train = mean_squared_error(y_train_column, y_train_pred)\n","\t\tmse_val = mean_squared_error(y_val_column, y_val_pred)\n","\n","\t\tdata[f\"{column} Training score MSE\"].append(mse_train)\n","\t\tdata[f\"{column} Validation score MSE\"].append(mse_val)\n","\n","\t\t# Calculate R2 for the training and validation sets\n","\t\tr2_train = r2_score(y_train_column, y_train_pred)\n","\t\tr2_val = r2_score(y_val_column, y_val_pred)\n","\n","\t\tdata[f\"{column} Training score R2\"].append(r2_train)\n","\t\tdata[f\"{column} Validation score R2\"].append(r2_val)\n","\t\tprint(f\"{column} Optimal parameters:\", data[f\"{column} Optimal parameters\"][-1])\n","\t\tprint(\"Time elapsed:\", time.time() - t)\n","\n","\n"],"metadata":{"id":"9GE3Jvj8d3_M","outputId":"05294e48-a245-450d-b11f-b232c553bf5c","colab":{"base_uri":"https://localhost:8080/","height":384},"executionInfo":{"status":"error","timestamp":1715412447835,"user_tz":-120,"elapsed":2290,"user":{"displayName":"Harm Tanis","userId":"15239638384700492892"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'cuml'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f1839872f9a9>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcuml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvergenceWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cuml'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]}]}